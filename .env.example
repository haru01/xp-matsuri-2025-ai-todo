# ==========================================
# フロントエンド設定 (VITE_* prefix)
# ==========================================

# WebSocket接続URL（フロントエンド用）
VITE_WS_URL=http://localhost:3002

# ==========================================
# サーバー設定
# ==========================================

# LLMプロバイダー設定
# 利用可能なプロバイダー: ollama, openai, anthropic, gemini (または google)
LLM_PROVIDER=ollama

# モデル名の設定
# Ollama: llama3.2:latest, llama3.1:8b, gemma3:27b, gpt-oss:20b, etc.
# OpenAI: gpt-5, gpt-4.5-turbo, o2-preview, o1
# Anthropic: claude-4-opus, claude-4-sonnet, claude-3.5-sonnet-latest
# Google/Gemini: gemini-2.5-pro, gemini-2.5-flash, gemini-1.5-pro, gemini-1.5-flash
# すべてstreamVNextメソッドでストリーミング対応
LLM_MODEL=llama3.2:latest

# 各プロバイダー用のAPIキー（必要に応じて設定）
# OpenAI を使用する場合
# OPENAI_API_KEY=your-openai-api-key-here

# Anthropic (Claude) を使用する場合
# ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Google (Gemini) を使用する場合
# GOOGLE_GENERATIVE_AI_API_KEY=your-google-api-key-here

# WebSocketサーバー設定
WS_PORT=3002
DEBUG_CONNECTIONS=false